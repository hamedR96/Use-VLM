{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "model_id=\"llava-hf/llama3-llava-next-8b-hf\"\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(model_id)\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#import torch\n",
    "#from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\n",
    "\n",
    "#MODEL_ID =\"google/paligemma2-3b-pt-448\"\n",
    "#DEVICE = torch.device(\"mps\")\n",
    "\n",
    "#processor = PaliGemmaProcessor.from_pretrained(MODEL_ID)\n",
    "#model = PaliGemmaForConditionalGeneration.from_pretrained(MODEL_ID).to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2db64070d16501c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for param in model.vision_tower.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.multi_modal_projector.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in model.language_model.parameters():\n",
    "    param.requires_grad = False "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "614a8a672c3f775b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Iterate through all parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Parameter Name: {name}\")\n",
    "        print(f\"Shape: {param.shape}\")\n",
    "        print(f\"Requires Grad: {param.requires_grad}\")\n",
    "        print(\"-\" * 50)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7565722845f3009"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#from peft import get_peft_model, LoraConfig\n",
    "\n",
    "#lora_config = LoraConfig(\n",
    "#     r=32,\n",
    "#     target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "#model = get_peft_model(model, lora_config)\n",
    "#model.print_trainable_parameters()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f0a177505d3efae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from MoLE import LoRA_MOE_LM\n",
    "\n",
    "class Args:\n",
    "    dense_moe = False  # Switch between dense and sparse routing for MoLE\n",
    "    lora_rank = 32\n",
    "    lora_alpha = 64\n",
    "    num_experts = 3\n",
    "\n",
    "args = Args()\n",
    "\n",
    "num_layers = len(model.language_model.model.layers)\n",
    "\n",
    "for i in range(num_layers):\n",
    "    original_mlp = model.language_model.model.layers[i].mlp\n",
    "    model.language_model.model.layers[i].mlp = LoRA_MOE_LM(args=args,\n",
    "                                                           lora_rank=args.lora_rank,\n",
    "                                                           lora_alpha=args.lora_alpha,\n",
    "                                                           num_experts=args.num_experts,\n",
    "                                                           original_module=original_mlp).to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bb67cd701092642"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Define the single training example\n",
    "url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "            {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "# Prepare inputs\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Define the target output (you must define the expected correct answer here)\n",
    "target_answer = \"The image shows a radar system.\"  # Example target output\n",
    "target_ids = processor.tokenizer(target_answer, return_tensors=\"pt\",).input_ids.to(device)\n",
    "\n",
    "# Fine-tuning parameters\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# Fine-tune the model\n",
    "model.train()\n",
    "epochs = 1  # Fine-tune for a single epoch for this test\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    outputs = model(**inputs,  labels=target_ids)\n",
    "    loss = outputs.loss\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e26661a3a74f1b8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
