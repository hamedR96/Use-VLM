{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "model_id=\"llava-hf/llama3-llava-next-8b-hf\"\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(model_id)\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from MoLE import LoRA_MOE_LM\n",
    "\n",
    "class Args:\n",
    "    dense_moe = False  # Switch between dense and sparse routing for MoLE\n",
    "    lora_rank = 4\n",
    "    lora_alpha = 1\n",
    "    num_experts = 4\n",
    "\n",
    "args = Args()\n",
    "\n",
    "def replace_with_mole_layers(module: Module, args: Args):\n",
    "    for name, child in module.named_children():\n",
    "        if hasattr(child, \"gate_proj\") and hasattr(child, \"down_proj\") and hasattr(child, \"up_proj\"):\n",
    "            setattr(module, name, LoRA_MOE_LM(args, args.lora_rank, args.lora_alpha, args.num_experts, child))\n",
    "        \n",
    "        else:\n",
    "            replace_with_mole_layers(child, args)\n",
    "    \n",
    "        \n",
    "replace_with_mole_layers(model, args)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bb67cd701092642"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Define the single training example\n",
    "url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "            {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "# Prepare inputs\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Define the target output (you must define the expected correct answer here)\n",
    "target_answer = \"The image shows a radar system.\"  # Example target output\n",
    "target_ids = processor.tokenizer(target_answer, return_tensors=\"pt\",).input_ids.to(device)\n",
    "\n",
    "# Fine-tuning parameters\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# Fine-tune the model\n",
    "model.train()\n",
    "epochs = 1  # Fine-tune for a single epoch for this test\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    outputs = model(**inputs,  labels=target_ids)\n",
    "    loss = outputs.loss\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e26661a3a74f1b8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
