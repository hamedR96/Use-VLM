{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dcee311-2665-4b4b-9ac4-08bc10aef900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "# Step 6: Upload to Hugging Face\n",
    "# Login to Hugging Face Hub\n",
    "login(token='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')  # Replace with your Hugging Face token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "499ff14c-684d-49dd-b3f6-d0aba1359562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d424da8d614f0ab34dc0870c8fc388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0f0262cff64194b92c2175e747021c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    PaliGemmaProcessor,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    ")\n",
    "from transformers.image_utils import load_image\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "model_id = \"google/paligemma2-3b-pt-448\"\n",
    "MODEL_ID=\"hamedrahimi/paligemma2_FairUser\"\n",
    "\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\").to(DEVICE).eval()\n",
    "fmodel = PaliGemmaForConditionalGeneration.from_pretrained(MODEL_ID, torch_dtype=torch.bfloat16, device_map=\"auto\").to(DEVICE).eval()\n",
    "\n",
    "processor = PaliGemmaProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff0629fc-5d89-4672-9dc2-834164e96ad9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`images` are expected as arguments to a `PaliGemmaProcessor` instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Leaving the prompt blank for pre-trained models\u001b[39;00m\n\u001b[1;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<image> tell me a joke\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbfloat16)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      7\u001b[0m input_len \u001b[38;5;241m=\u001b[39m model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/paligemma/processing_paligemma.py:255\u001b[0m, in \u001b[0;36mPaliGemmaProcessor.__call__\u001b[0;34m(self, images, text, audio, videos, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m return_token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`images` are expected as arguments to a `PaliGemmaProcessor` instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using PaliGemma without a text prefix. It will perform as a picture-captioning model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    259\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: `images` are expected as arguments to a `PaliGemmaProcessor` instance."
     ]
    }
   ],
   "source": [
    "url = \"https://media.licdn.com/dms/image/v2/D4E03AQGGw88ch9oKDg/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1731376120703?e=1739404800&v=beta&t=Wq-7gMt9EOGwoGoK3k8JTqHceIG1X59MMwg4u9PApKE\"\n",
    "image = load_image(url)\n",
    "\n",
    "# Leaving the prompt blank for pre-trained models\n",
    "prompt = \"<image> tell me a joke\"\n",
    "model_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(torch.bfloat16).to(model.device)\n",
    "input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "    decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "    print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7b8d97c-0eb4-4dbf-9399-dd3835491023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a picture of me\n"
     ]
    }
   ],
   "source": [
    "url = \"https://media.licdn.com/dms/image/v2/D4E03AQGGw88ch9oKDg/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1731376120703?e=1739404800&v=beta&t=Wq-7gMt9EOGwoGoK3k8JTqHceIG1X59MMwg4u9PApKE\"\n",
    "image = load_image(url)\n",
    "\n",
    "# Leaving the prompt blank for pre-trained models\n",
    "prompt = \"<image> user profile en\"\n",
    "model_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(torch.bfloat16).to(model.device)\n",
    "input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = fmodel.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "    decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "    print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30be16ee-0b3e-4398-9cb0-69377141030d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black apple icon on white background .\n"
     ]
    }
   ],
   "source": [
    "url = \"https://media.istockphoto.com/id/1346182204/fr/vectoriel/icône-apple-silhouette-noire.jpg?s=1024x1024&w=is&k=20&c=sBN3i0KbykMgYKAhXmxbqWrgyDd-fgkimMYGiVz-WYw=\"\n",
    "image = load_image(url)\n",
    "\n",
    "# Leaving the prompt blank for pre-trained models\n",
    "prompt = \"<image>\"\n",
    "model_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(torch.bfloat16).to(model.device)\n",
    "input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "    decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "    print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70e894b8-4e07-482d-98e1-f2ace25aef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple icon black\n"
     ]
    }
   ],
   "source": [
    "url = \"https://media.istockphoto.com/id/1346182204/fr/vectoriel/icône-apple-silhouette-noire.jpg?s=1024x1024&w=is&k=20&c=sBN3i0KbykMgYKAhXmxbqWrgyDd-fgkimMYGiVz-WYw=\"\n",
    "image = load_image(url)\n",
    "\n",
    "# Leaving the prompt blank for pre-trained models\n",
    "prompt = \"<image>\"\n",
    "model_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(torch.bfloat16).to(model.device)\n",
    "input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = fmodel.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "    decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "    print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9296bf6d-a992-41af-a7d3-0341bb190797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
